# SAliCLIP
The advent of the Transformer architecture, followed by the development of contrastive models like OpenAIâ€™s CLIP, has enabled learning robust latent space representations of data in multiple modalities such as text and image. In this work, we employ knowledge distillation to adapt an audio encoder to the embedding space of CLIP. Aligning the embedding spaces of image, speech, and text imparts additional functionality to CLIP, which can be leveraged for various downstream tasks. We utilize this framework of encoders to guide the images generated by a GAN-based network to represent any given speech signal. The proposed architecture achieves new state-of-the-art retrieval and classification accuracies on the Spoken-COCO dataset. We further demonstrate its ability to perform classification and cross-modal retrieval with zero-shot learning and fine-tuning on multiple datasets.
## Installation
```Python 3``` or higher is recommended. We encorage that you use a python virtual enviornment to run SAliCLIP.
### Installing required libraries
```
pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu113
```
### Enviornment Setup
Before running the below command make sure your system has [git](https://www.atlassian.com/git/tutorials/install-git) and [curl](https://help.ubidots.com/en/articles/2165289-learn-how-to-install-run-curl-on-windows-macosx-linux) commands installed.
```
python3 envSetup.py
```
## Running SAliCLIP
```
streamlit run main_vq.py
```

# SAliCLIP
The advent of the Transformer architecture, followed by the development of contrastive models like OpenAIâ€™s CLIP, has enabled learning robust latent space representations of data in multiple modalities such as text and image. In this work, we employ knowledge distillation to adapt an audio encoder to the embedding space of CLIP. Aligning the embedding spaces of image, speech, and text imparts additional functionality to CLIP, which can be leveraged for various downstream tasks. We utilize this framework of encoders to guide the images generated by a GAN-based network to represent any given speech signal. The proposed architecture achieves new state-of-the-art retrieval and classification accuracies on the Spoken-COCO dataset. We further demonstrate its ability to perform classification and cross-modal retrieval with zero-shot learning and fine-tuning on multiple datasets.

## Config file
The config file contains multiple paramaters which are required to run SAliCLIP. The Device that has to be used can be changed in the config file. By default the device that will be used is cpu. This can be changed by maiking device = cuda for GPU . </br>
**Note** : Check the hardware requirements before doing this change.
## Installation
```Python 3``` or higher is recommended. We encorage that you use a python virtual enviornment to run SAliCLIP.
### Installing required libraries
For linux and windows
```
pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu113
```
For Mac
```
pip install -r requirementsMac.txt 
```
### Enviornment Setup
Before running the below command make sure your system has [git](https://www.atlassian.com/git/tutorials/install-git) and [curl](https://help.ubidots.com/en/articles/2165289-learn-how-to-install-run-curl-on-windows-macosx-linux) commands installed.
```
python3 EnvSetup.py
```
## Running SAliCLIP
```
streamlit run Main.py
```
## Hardware Requirements
### CPU Requirements
16 GB RAM 
### GPU Requirements
25 GB RAM
## Interface
Once the Main file is run the following interface will open up.
</br>
[Add image]
</br>
In the **Video Generator** tab, a user can input multiple audio files up to 200MB. Once an audio is uploaded it can be played in the interface. Then the user is given a choice to change some video generating parameters such as step size, cuts, number of iterations, break audio and BRISQUE.</br> 
+ Step Size - This is the learning rate for the VQGAN Model .
+ Cuts - The number of cuts to perform in the generated image before sending it to the image encoder.
+ Number of Iterations - The number of times an output image is generated, the loss is calculated between the output image and the input audio, the model is back propagated such that the input is changed in the latent space to generate a better image.
+ Break Audio - Selecting this option breaks the input audio into 2 parts and loss is calculated separately for these 2 parts and is added to the total loss of the audio and image. Less weight is given to these 2 losses, this is because the overall context of the audio should not be lost.
+ BRISQUE - Blind Reference Image Spatial Quality Estimator(BRISQUE) for the generated image at each epoch. A BRISQUE score of 0 indicates the highest quality image whereas a BRISQUE score of 150 indicates an extremely poor quality image, it takes into account factors such as sharpness of features, image clarity, pixel distribution, etc. to judge the quality of an image.</br>
</br>
These audio clips are finally converted to a video which is later displayed with the audio of the corresponding  audio clip.</br>

In the **Image Classification** tab, a user can input an image upto 200MB. Once the image is uploaded, the model gives an ouptut of what  

